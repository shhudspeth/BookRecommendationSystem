{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Recommender System that uses Surprise and Custom Book Ratings Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is one my must have books. It is a master...</td>\n",
       "      <td>12 11, 2003</td>\n",
       "      <td>A2S166WSCFIFP5</td>\n",
       "      <td>adead_poet@hotmail.com \"adead_poet@hotmail.com\"</td>\n",
       "      <td>close to god</td>\n",
       "      <td>1071100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This book provides a reflection that you can a...</td>\n",
       "      <td>01 18, 2014</td>\n",
       "      <td>A1BM81XB4QHOA3</td>\n",
       "      <td>Ahoro Blethends \"Seriously\"</td>\n",
       "      <td>Must Read for Life Afficianados</td>\n",
       "      <td>1390003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I first read THE PROPHET in college back in th...</td>\n",
       "      <td>09 27, 2011</td>\n",
       "      <td>A1MOSTXNIO5MPJ</td>\n",
       "      <td>Alan Krug</td>\n",
       "      <td>Timeless for every good and bad time in your l...</td>\n",
       "      <td>1317081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[7, 9]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A timeless classic.  It is a very demanding an...</td>\n",
       "      <td>10 7, 2002</td>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>Alaturka</td>\n",
       "      <td>A Modern Rumi</td>\n",
       "      <td>1033948800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reading this made my mind feel like a still po...</td>\n",
       "      <td>01 27, 2014</td>\n",
       "      <td>A3V1MKC2BVWY48</td>\n",
       "      <td>Alex Dawson</td>\n",
       "      <td>This book will bring you peace</td>\n",
       "      <td>1390780800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        asin helpful  overall  \\\n",
       "0           0  000100039X  [0, 2]      5.0   \n",
       "1           1  000100039X  [0, 0]      5.0   \n",
       "2           2  000100039X  [0, 0]      5.0   \n",
       "3           3  000100039X  [7, 9]      5.0   \n",
       "4           4  000100039X  [0, 0]      5.0   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  This is one my must have books. It is a master...  12 11, 2003   \n",
       "1  This book provides a reflection that you can a...  01 18, 2014   \n",
       "2  I first read THE PROPHET in college back in th...  09 27, 2011   \n",
       "3  A timeless classic.  It is a very demanding an...   10 7, 2002   \n",
       "4  Reading this made my mind feel like a still po...  01 27, 2014   \n",
       "\n",
       "       reviewerID                                     reviewerName  \\\n",
       "0  A2S166WSCFIFP5  adead_poet@hotmail.com \"adead_poet@hotmail.com\"   \n",
       "1  A1BM81XB4QHOA3                      Ahoro Blethends \"Seriously\"   \n",
       "2  A1MOSTXNIO5MPJ                                        Alan Krug   \n",
       "3  A2XQ5LZHTD4AFT                                         Alaturka   \n",
       "4  A3V1MKC2BVWY48                                      Alex Dawson   \n",
       "\n",
       "                                             summary  unixReviewTime  \n",
       "0                                       close to god      1071100800  \n",
       "1                    Must Read for Life Afficianados      1390003200  \n",
       "2  Timeless for every good and bad time in your l...      1317081600  \n",
       "3                                      A Modern Rumi      1033948800  \n",
       "4                     This book will bring you peace      1390780800  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "read_ = pd.read_csv('books.csv')\n",
    "\n",
    "read_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_.drop(columns = ['Unnamed: 0'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_ = read_[['reviewerID', 'asin', 'overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A2S166WSCFIFP5</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A1BM81XB4QHOA3</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A1MOSTXNIO5MPJ</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A3V1MKC2BVWY48</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall\n",
       "0  A2S166WSCFIFP5  000100039X      5.0\n",
       "1  A1BM81XB4QHOA3  000100039X      5.0\n",
       "2  A1MOSTXNIO5MPJ  000100039X      5.0\n",
       "3  A2XQ5LZHTD4AFT  000100039X      5.0\n",
       "4  A3V1MKC2BVWY48  000100039X      5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_ave= read_.groupby('reviewerID')['overall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_count = read_.groupby('reviewerID')['overall'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewerID\n",
       "A00098481E8WIB9X660MV    1\n",
       "A00109803PZJ91RLT7DPN    2\n",
       "A00154983C0WD3K7FLYTW    1\n",
       "A00311082372LV4OV6WJC    1\n",
       "A00338282E99B8OR2JYTZ    3\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    317842.000000\n",
       "mean          3.146214\n",
       "std           9.883328\n",
       "min           1.000000\n",
       "25%           1.000000\n",
       "50%           2.000000\n",
       "75%           3.000000\n",
       "max        2567.000000\n",
       "Name: overall, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates a dictionary of threshold values for each user based on their average rating counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def user_ave_rating_threshold(df, n):\n",
    "    ''' Function that averages a unique reviewerID's ratings if they have reviewed more than n books\n",
    "        and returns a dictionary of threshold values and ids'''\n",
    "    \n",
    "    \n",
    "    threshold_dict = defaultdict(list)\n",
    "    \n",
    "    # note--> to optimize only find the mean if it means the user_Rating_count\n",
    "    \n",
    "    user_ratings_ave= df.groupby('reviewerID')['overall'].mean()\n",
    "    user_ratings_count = df.groupby('reviewerID')['overall'].count()\n",
    "    \n",
    "    for x in df['reviewerID'].unique():\n",
    "        if user_ratings_count[x] > n:\n",
    "            threshold_dict[x] = user_ratings_ave[x]\n",
    "        else:\n",
    "            threshold_dict[x] = 3.5\n",
    "    \n",
    "    return(threshold_dict)\n",
    "            \n",
    "        \n",
    "threshold_diction = user_ave_rating_threshold(read_, 8)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some troubleshooting support for importing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/shhudspeth/sharpest_minds/book_rec_sys',\n",
       " '/Users/shhudspeth/open_CV_env/lib/python37.zip',\n",
       " '/Users/shhudspeth/open_CV_env/lib/python3.7',\n",
       " '/Users/shhudspeth/open_CV_env/lib/python3.7/lib-dynload',\n",
       " '/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7',\n",
       " '',\n",
       " '/Users/shhudspeth/open_CV_env/lib/python3.7/site-packages',\n",
       " '/Users/shhudspeth/open_CV_env/lib/python3.7/site-packages/google_cloud_vision-0.39.0-py3.7.egg',\n",
       " '/Users/shhudspeth/zhenglib/bookshelf',\n",
       " '/Users/shhudspeth/open_CV_env/lib/python3.7/site-packages',\n",
       " '/Users/shhudspeth/open_CV_env/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/Users/shhudspeth/.ipython']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Surprise sometimes does not import, have to check jupyter kernal\n",
    "import sys\n",
    "\n",
    "sys.executable\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Load Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset\n",
    "\n",
    "# Define the format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Load the data from the file using the reader format\n",
    "data = Dataset.load_from_df(read_, reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Baseline RSME using NormalPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "from surprise import NormalPredictor\n",
    "\n",
    "\n",
    "norm_preds = cross_validate(NormalPredictor(), data, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.41290067, 1.41734471, 1.41214729, 1.41727565, 1.41550654]),\n",
       " 'test_mae': array([1.0748451 , 1.07826703, 1.07223805, 1.07767175, 1.07613188]),\n",
       " 'fit_time': (4.3955161571502686,\n",
       "  3.2166762351989746,\n",
       "  5.770097970962524,\n",
       "  3.4375648498535156,\n",
       "  4.69743013381958),\n",
       " 'test_time': (7.963748931884766,\n",
       "  8.849919080734253,\n",
       "  13.875865697860718,\n",
       "  3.911918878555298,\n",
       "  3.9238688945770264)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use KFold&SVD to compare to NormalPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0095\n",
      "MAE:  0.7750\n",
      "RMSE: 1.0077\n",
      "MAE:  0.7742\n",
      "RMSE: 1.0107\n",
      "MAE:  0.7762\n",
      "RMSE: 1.0125\n",
      "MAE:  0.7772\n",
      "RMSE: 1.0098\n",
      "MAE:  0.7756\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import KFold\n",
    "\n",
    "rmse_to_ave= []\n",
    "mae_to_ave = []\n",
    "\n",
    "# define a cross-validation iterator\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "algo = SVD()\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error and Mean Average Error\n",
    "    rmse_to_ave.append(accuracy.rmse(predictions, verbose=True))\n",
    "    mae_to_ave.append(accuracy.mae(predictions, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_ave = sum(rmse_to_ave)/len(rmse_to_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_ave = sum(mae_to_ave)/len(mae_to_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change of rmse is 0.28620952275958556 and change of mae is 0.2790322209682031\n"
     ]
    }
   ],
   "source": [
    "#average improvements:\n",
    "improve_rmse = (norm_preds['test_rmse'].mean() - rmse_ave.mean() )/norm_preds['test_rmse'].mean()\n",
    "improve_mae = (norm_preds['test_mae'].mean() - mae_ave.mean() )/norm_preds['test_mae'].mean()\n",
    "\n",
    "print(\"change of rmse is {0} and change of mae is {1}\".format(improve_rmse,improve_mae ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION: KFold is an improvement over NormalPredictor by about 30% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Top N Recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "\n",
    "# First train an SVD algorithm on the movielens dataset.\n",
    "trainset = data.build_full_trainset()\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Precision and Recall Metrics Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a base threshold at 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "algo = SVD()\n",
    "\n",
    "prec_to_ave = []\n",
    "rec_to_ave = []\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    \n",
    "    prec_to_ave.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    rec_to_ave.append(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_tpr_fpr(predictions, threshold=3.5):\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    pred_df['r_ui'].where(pred_df['r_ui']>threshold, 1, inplace=True)\n",
    "    pred_df['r_ui'].where(pred_df['r_ui']<=threshold, 0, inplace=True)\n",
    "    \n",
    "    pred_df['est'].where(pred_df['est']>threshold, 1, inplace=True)\n",
    "    pred_df['est'].where(pred_df['est']<=threshold, 0, inplace=True)\n",
    "\n",
    "    return pred_df['r_ui'], pred_df['est']\n",
    "\n",
    "true_r, est = make_binary_tpr_fpr(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-c54378e98a0b>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-c54378e98a0b>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    plt.show()\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "\n",
    "def graph_roc (true_r, est):\n",
    "    fpr, tpr, thresholds = roc_curve(true_r, est)\n",
    "    auc_ = auc(fpr, tpr)\n",
    "    plt.plot(fpr,tpr, '-')\n",
    "    plt.ylabel(\"FPR/Precision Scores\")\n",
    "    plt.xlabel(\"TPR/Recall Scores\")\n",
    "    plt.title(\"ROC Scores and Auc of \" + str(round(auc_, 2)))\n",
    "    plt.show()\n",
    "    \n",
    "graph_roc(true_r, est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_average = sum(prec_to_ave)/len(prec_to_ave)\n",
    "recall_average = sum(rec_to_ave)/len(prec_to_ave)\n",
    "\n",
    "print(\"Precision and Recall averages are {0} and {1}, respectively\".format(precision_average, recall_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall values using User-Average Ratings as a Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, threshold_dictionary, k=10):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        threshold = threshold_dictionary[uid]\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "algo = SVD()\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, threshold_diction, k=5)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "  \n",
    "    prec_to_ave.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    rec_to_ave.append(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_average = sum(prec_to_ave)/len(prec_to_ave)\n",
    "recall_average = sum(rec_to_ave)/len(prec_to_ave)\n",
    "\n",
    "print(\"Precision and Recall averages are {0} and {1}, respectively\".format(precision_average, recall_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_tpr_fpr_user(predictions, threshold_diction):\n",
    "    \n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    pred_df['threshold'] = pred_df['uid'].map(threshold_diction)\n",
    "    pred_df['r_ui'].where(pred_df['r_ui']>=pred_df['threshold'], 1, inplace=True)\n",
    "    pred_df['r_ui'].where(pred_df['r_ui']<pred_df['threshold'], 0, inplace=True)\n",
    "    pred_df['est'].where(pred_df['est']>=pred_df['threshold'], 1, inplace=True)\n",
    "    pred_df['est'].where(pred_df['est']<pred_df['threshold'], 0, inplace=True)\n",
    "    \n",
    "    return pred_df['r_ui'], pred_df['est']\n",
    "\n",
    "true_r_ut, est_ut = make_binary_tpr_fpr_user(predictions, threshold_diction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_roc(true_r_ut, est_ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
